{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian',\n",
       " 'Ukranian',\n",
       " 'Polish',\n",
       " 'noise_data',\n",
       " 'Portuguese',\n",
       " 'French',\n",
       " 'Arabic',\n",
       " 'Albanian',\n",
       " 'INCAS',\n",
       " 'stormfront',\n",
       " 'English',\n",
       " 'AGNEWS',\n",
       " 'personal_attack',\n",
       " 'Turkish',\n",
       " 'Greek',\n",
       " 'Italian',\n",
       " 'Estonian',\n",
       " 'IMDB',\n",
       " 'Jigsaw Annotations',\n",
       " 'Hindi',\n",
       " 'Latvian',\n",
       " 'Danish',\n",
       " 'ucc',\n",
       " 'Chinese',\n",
       " 'German',\n",
       " 'drive-download-20240129T222601Z-001.zip',\n",
       " 'GHC']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albanian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lule e eger e rrazikshme,ato frutat kan helm.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kungull eshte</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kungull</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iriq Bio</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eshte kungull</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11869</th>\n",
       "      <td>O gjigand i skenes na knaqe me humorin tend je...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11870</th>\n",
       "      <td>https://soundcloud.com/dennisskendo</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11871</th>\n",
       "      <td>Nuk je artist</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11872</th>\n",
       "      <td>Hallall te qofte se tregon te verteten ....</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11873</th>\n",
       "      <td>O ermal vetem per barsaletat esht nona ne ship...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11874 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  subtask_a  \\\n",
       "0          Lule e eger e rrazikshme,ato frutat kan helm.          1   \n",
       "1                                          Kungull eshte          1   \n",
       "2                                                Kungull          1   \n",
       "3                                               Iriq Bio          1   \n",
       "4                                          Eshte kungull          1   \n",
       "...                                                  ...        ...   \n",
       "11869  O gjigand i skenes na knaqe me humorin tend je...          1   \n",
       "11870                https://soundcloud.com/dennisskendo          1   \n",
       "11871                                      Nuk je artist          1   \n",
       "11872        Hallall te qofte se tregon te verteten ....          1   \n",
       "11873  O ermal vetem per barsaletat esht nona ne ship...          1   \n",
       "\n",
       "       subtask_b  subtask_c  \n",
       "0              2          3  \n",
       "1              2          3  \n",
       "2              2          3  \n",
       "3              2          3  \n",
       "4              2          3  \n",
       "...          ...        ...  \n",
       "11869          2          3  \n",
       "11870          2          3  \n",
       "11871          2          3  \n",
       "11872          2          3  \n",
       "11873          2          3  \n",
       "\n",
       "[11874 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"Albanian\", \"Albanian_1_train.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.rename(columns={\"subtask_a\": \"offensive\", \"text\": \"text\"}, inplace=True)\n",
    "\n",
    "\n",
    "stratify_column = \"offensive\"\n",
    "\n",
    "# Concatenate the data frames\n",
    "all_data = df\n",
    "\n",
    "# Stratified split into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(all_data, test_size=0.2, stratify=all_data[stratify_column], random_state=42)\n",
    "valid_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data[stratify_column], random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(DATA_DIR, \"Albanian\", \"train.csv\"), index=False)\n",
    "test_data.to_csv(os.path.join(DATA_DIR, \"Albanian\", \"test.csv\"), index=False)\n",
    "valid_data.to_csv(os.path.join(DATA_DIR, \"Albanian\", \"dev.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_albanian_data():\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Albanian\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Albanian\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Albanian\", \"dev.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"valid\": valid_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATA_DIR, \"Danish\", \"Danish_1_train.csv\"))\n",
    "train_data[\"offensive\"] = train_data[\"label\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.11, stratify=train_data[\"offensive\"], random_state=42)\n",
    "\n",
    "train_data.to_csv(os.path.join(DATA_DIR, \"Danish\", \"train.csv\"), index=False)\n",
    "val_data.to_csv(os.path.join(DATA_DIR, \"Danish\", \"val.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_danish_data():\n",
    "    def fix_df(df):\n",
    "        df[\"offensive\"] = train_data[\"label\"].apply(lambda x: 1 if x == \"OFF\" else 0)\n",
    "        return df[[\"offensive\", \"text\"]]\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Danish\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Danish\", \"Danish_1_test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Danish\", \"val.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"val\": valid_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jeg skal gøre mig umage når jeg koger pasta senere'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_danish_data()[\"test\"][\"text\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2755317/920735770.py:1: DtypeWarning: Columns (0,1,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(DATA_DIR, \"Estonian\", \"Estonian_3_train.csv\"))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"Estonian\", \"Estonian_3_train.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"modereted_by\"] = df.modereted_by.fillna(0)\n",
    "df.modereted_by = df.modereted_by.replace(\"\\\\N\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"moderated\"] = df.modereted_by.apply(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moderated\n",
       "0    1002478\n",
       "1     537242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.moderated.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"moderated\"\n",
    "\n",
    "train_df, rest_df = train_test_split(df, test_size=0.2, stratify=df[label_col], random_state=42)\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5, stratify=rest_df[label_col], random_state=42)\n",
    "_ , val_df = train_test_split(val_df, test_size=5000, stratify=val_df[label_col], random_state=42)\n",
    "_ , test_df = train_test_split(test_df, test_size=5000, stratify=test_df[label_col], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"content\", \"moderated\"]].reset_index().to_csv(os.path.join(DATA_DIR, \"Estonian\", \"train.csv\"), index=False)\n",
    "val_df[[\"content\", \"moderated\"]].reset_index().to_csv(os.path.join(DATA_DIR, \"Estonian\", \"val.csv\"), index=False)\n",
    "test_df[[\"content\", \"moderated\"]].reset_index().to_csv(os.path.join(DATA_DIR, \"Estonian\", \"test.csv\"), index=False)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_estonian_data():\n",
    "    def fix_df(df):\n",
    "        df.rename(columns={\"content\": \"text\"}, inplace=True)\n",
    "        return df\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Estonian\", \"train.csv\")).reset_index\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Estonian\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Estonian\", \"val.csv\"))\n",
    "    return {\"train\": fix_df(train_data), \"test\": fix_df(test_data), \"val\": fix_df(valid_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>moderated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nöus sinuga ... samas tekib mul ka kysimus , m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Тех кто кушает мацу! Узнаю я по лицу &amp;#x1F923</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eks see mingitmoodi haige inimene. Kaastunne t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eks nad ole paanikas, et bitcoin ära vajub. Ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Слава Путину! Бьёт мразей их же оружием</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>EAS raha saab tagasi nõuda ilma piiblita.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Käi kanni oma vabamüürlusega debiil.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Seksuaalfunktsioone ei saa käsitleda inimesest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Vaatasin: Kunnas EKRE-st jättis möku mulje. Mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Lugedes sinu teksti, ei teki küll selle teksti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  moderated\n",
       "0     nöus sinuga ... samas tekib mul ka kysimus , m...          1\n",
       "1         Тех кто кушает мацу! Узнаю я по лицу &#x1F923          1\n",
       "2     Eks see mingitmoodi haige inimene. Kaastunne t...          1\n",
       "3     Eks nad ole paanikas, et bitcoin ära vajub. Ha...          0\n",
       "4              Слава Путину! Бьёт мразей их же оружием           0\n",
       "...                                                 ...        ...\n",
       "4995      EAS raha saab tagasi nõuda ilma piiblita.....          1\n",
       "4996               Käi kanni oma vabamüürlusega debiil.          0\n",
       "4997  Seksuaalfunktsioone ei saa käsitleda inimesest...          1\n",
       "4998  Vaatasin: Kunnas EKRE-st jättis möku mulje. Mi...          0\n",
       "4999  Lugedes sinu teksti, ei teki küll selle teksti...          0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_estonian_data()[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoderated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodereted_by\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noise/noise_env/lib/python3.10/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noise/noise_env/lib/python3.10/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noise/noise_env/lib/python3.10/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/noise/noise_env/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noise/noise_env/lib/python3.10/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[106], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoderated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodereted_by\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not str"
     ]
    }
   ],
   "source": [
    "df[\"moderated\"]=df[\"modereted_by\"].apply(lambda x: 0 if x == \"\\\\N\" or math.isnan(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"German\", \"German_1_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['Sexism Count Crowd', 'Racism Count Crowd',\n",
    "       'Threat Count Crowd', 'Insult Count Crowd', 'Profanity Count Crowd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in target_cols:\n",
    "    df[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2711058823529412"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"offensive\"] = df[target_cols].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)\n",
    "sum(df[\"offensive\"])/len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"Text\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"offensive\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offensive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Niemand braucht Laschet den Merkel Günstling !...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>das war apokalypse now. nicht einmal zu einem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Katastrophal  -  Katastrophal -  anders kann d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Dann sollten wir unsere Rüstungsexporte schnel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>na ja,im notfall sind wir amis ja noch da um z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84995</th>\n",
       "      <td>1</td>\n",
       "      <td>Mein Gott!! Was für ein entsetzliches Gequatsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84996</th>\n",
       "      <td>1</td>\n",
       "      <td>Der hat halt auch nichts in der hohlen Birne!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84997</th>\n",
       "      <td>0</td>\n",
       "      <td>Blabla, nehme das Auto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84998</th>\n",
       "      <td>0</td>\n",
       "      <td>sack reis in china umgefallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84999</th>\n",
       "      <td>1</td>\n",
       "      <td>Na, dann bin ich ja beruhigt, dass Frl. Julia ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       offensive                                               text\n",
       "0              0  Niemand braucht Laschet den Merkel Günstling !...\n",
       "1              0  das war apokalypse now. nicht einmal zu einem ...\n",
       "2              1  Katastrophal  -  Katastrophal -  anders kann d...\n",
       "3              0  Dann sollten wir unsere Rüstungsexporte schnel...\n",
       "4              0  na ja,im notfall sind wir amis ja noch da um z...\n",
       "...          ...                                                ...\n",
       "84995          1  Mein Gott!! Was für ein entsetzliches Gequatsc...\n",
       "84996          1  Der hat halt auch nichts in der hohlen Birne!!!!!\n",
       "84997          0                          Blabla, nehme das Auto...\n",
       "84998          0                   sack reis in china umgefallen...\n",
       "84999          1  Na, dann bin ich ja beruhigt, dass Frl. Julia ...\n",
       "\n",
       "[85000 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"offensive\"\n",
    "train_df, rest_df = train_test_split(df, test_size=0.2, stratify=df[label_col], random_state=42)\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5, stratify=rest_df[label_col], random_state=42)\n",
    "_ , val_df = train_test_split(val_df, test_size=5000, stratify=val_df[label_col], random_state=42)\n",
    "_ , test_df = train_test_split(test_df, test_size=5000, stratify=test_df[label_col], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DATA_DIR, \"German\", \"train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(DATA_DIR, \"German\", \"val.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(DATA_DIR, \"German\", \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_german_data():\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"German\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"German\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"German\", \"val.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"val\": valid_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"Greek\", \"Greek_1_train.csv\")).rename(columns={\"subtask_a\": \"offensive\"})\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"Greek\", \"Greek_1_test.csv\")).rename(columns={\"subtask_a\": \"offensive\"})\n",
    "\n",
    "df = pd.concat([train_df, test_df])\n",
    "\n",
    "\n",
    "label_col = \"offensive\"\n",
    "train_df, rest_df = train_test_split(df, test_size=0.2, stratify=df[label_col], random_state=42)\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5, stratify=rest_df[label_col], random_state=42)\n",
    "# _ , val_df = train_test_split(val_df, test_size=5000, stratify=val_df[label_col], random_state=42)\n",
    "# _ , test_df = train_test_split(test_df, test_size=5000, stratify=test_df[label_col], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna().to_csv(os.path.join(DATA_DIR, \"Greek\", \"train.csv\"), index=False)\n",
    "val_df.dropna().to_csv(os.path.join(DATA_DIR, \"Greek\", \"val.csv\"), index=False)\n",
    "test_df.dropna().to_csv(os.path.join(DATA_DIR, \"Greek\", \"test.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2653061224489796"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_greek_data():\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Greek\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Greek\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Greek\", \"val.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"val\": valid_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"Italian\", \"Italian_1_train.csv\"))\n",
    "train_df.rename(columns={\"label\": \"hate\"}, inplace=True)\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"Italian\", \"Italian_1_test.csv\"))\n",
    "test_df.rename(columns={\"label\": \"hate\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, rest_df = train_test_split(train_df, test_size=0.2, stratify=train_df[\"hate\"], random_state=42)\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5, stratify=rest_df[\"hate\"], random_state=42)\n",
    "\n",
    "train_df.dropna().to_csv(os.path.join(DATA_DIR, \"Italian\", \"train.csv\"), index=False)\n",
    "val_df.dropna().to_csv(os.path.join(DATA_DIR, \"Italian\", \"val.csv\"), index=False)\n",
    "test_df.dropna().to_csv(os.path.join(DATA_DIR, \"Italian\", \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_italian_data():\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Italian\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Italian\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Italian\", \"val.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"val\": valid_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latvian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2847488/3138504868.py:1: DtypeWarning: Columns (0,1,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(DATA_DIR, \"Latvian\", \"Latvian_2_train.csv\"))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"Latvian\", \"Latvian_2_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modereted_by\n",
       "0                                                                                                                  2862668\n",
       "System                                                                                                              293121\n",
       "55ef51c35b7e218b34dd47605cfe5d6a                                                                                    100178\n",
       "5d7b9adcbe1c629ec722529dd12e5129                                                                                     81404\n",
       "252e691406782824eec43d7eadc3d256                                                                                     41219\n",
       "571b03453706e700642c6f448718d198                                                                                       331\n",
       "d92db5e4b5bc9eae0e95e273326064c1                                                                                       280\n",
       "16bb98135768765808478a26c61e26b3                                                                                        70\n",
       "ff1eedb125eff2589fb3a35f1a7c6e8d                                                                                        51\n",
       "459016cd71a933aac4b5d739d19cd98c                                                                                        36\n",
       "b9ea889c6fc3fb2b4c343d7400734856                                                                                        32\n",
       "6cc5d4d5f692ea81c1baddf5e998f049                                                                                        31\n",
       "2eaa24709de855c79ace0e26c7fa8ad4                                                                                        29\n",
       "295aaa31a56f5e054fcfc7b9fdd05bf2                                                                                        23\n",
       "3071239e893d3b505ec1b0db4b90ea58                                                                                         4\n",
       "388411e626342a6deb8900e90f68de39                                                                                         3\n",
       "716890265e6c69c99bad97217356a57e                                                                                         3\n",
       "0267aaf632e87a63288a08331f22c7c3                                                                                         2\n",
       "66d8c0f4ab3c3681ff3f9093b05136a6                                                                                         2\n",
       "2cc51c3ea088939d879bae1092a7ed0b                                                                                         1\n",
       "046be5694dce5d772d74b8f6964149a2                                                                                         1\n",
       " большинству людей сталинская система обеспечила возможность продвижения вверх и участия в общественной жизни.\\          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"modereted_by\"] = df.modereted_by.fillna(0)\n",
    "df.modereted_by = df.modereted_by.replace(\"\\\\N\", 0)\n",
    "\n",
    "df.modereted_by.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"moderated\"] = df.modereted_by.apply(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moderated\n",
       "0    2862668\n",
       "1     516822\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.moderated.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"moderated\"\n",
    "train_df, rest_df = train_test_split(df, test_size=0.2, stratify=df[label_col], random_state=42)\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5, stratify=rest_df[label_col], random_state=42)\n",
    "_ , val_df = train_test_split(val_df, test_size=5000, stratify=val_df[label_col], random_state=42)\n",
    "_ , test_df = train_test_split(test_df, test_size=5000, stratify=test_df[label_col], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"content\", \"moderated\"]].dropna().to_csv(os.path.join(DATA_DIR, \"Latvian\", \"train.csv\"), index=False)\n",
    "val_df[[\"content\", \"moderated\"]].dropna().to_csv(os.path.join(DATA_DIR, \"Latvian\", \"val.csv\"), index=False)\n",
    "test_df[[\"content\", \"moderated\"]].dropna().to_csv(os.path.join(DATA_DIR, \"Latvian\", \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latvian_data():\n",
    "    def fix_df(df):\n",
    "        df.rename(columns={\"content\": \"text\"}, inplace=True)\n",
    "        return df\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Latvian\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Latvian\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Latvian\", \"val.csv\"))\n",
    "    return {\"train\": fix_df(train_data), \"test\": fix_df(test_data), \"val\": fix_df(valid_data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>homophobia</th>\n",
       "      <th>obscene</th>\n",
       "      <th>insult</th>\n",
       "      <th>racism</th>\n",
       "      <th>misogyny</th>\n",
       "      <th>xenophobia</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meu nivel de amizade com isis é ela ter meu in...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt @user @user o cara adultera dados, que fora...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user @user o cara só é simplesmente o m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu to chorando vei vsf e eu nem staneio izone ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eleitor do Bolsonaro é tão ignorante q não per...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>@user faz favor vai carai</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>só queria conhecer alguém que não conhece o he...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>vcs militam na hora errada em cima de memes, p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>@user porra any eu tava c dor de cabeca e fui ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>saudades da iasmin de 2017 eu era gótica demai...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  homophobia  obscene  \\\n",
       "0      Meu nivel de amizade com isis é ela ter meu in...         0.0      0.0   \n",
       "1      rt @user @user o cara adultera dados, que fora...         0.0      0.0   \n",
       "2      @user @user @user o cara só é simplesmente o m...         0.0      2.0   \n",
       "3      eu to chorando vei vsf e eu nem staneio izone ...         0.0      1.0   \n",
       "4      Eleitor do Bolsonaro é tão ignorante q não per...         0.0      1.0   \n",
       "...                                                  ...         ...      ...   \n",
       "20995                          @user faz favor vai carai         0.0      0.0   \n",
       "20996  só queria conhecer alguém que não conhece o he...         1.0      0.0   \n",
       "20997  vcs militam na hora errada em cima de memes, p...         0.0      0.0   \n",
       "20998  @user porra any eu tava c dor de cabeca e fui ...         0.0      0.0   \n",
       "20999  saudades da iasmin de 2017 eu era gótica demai...         0.0      0.0   \n",
       "\n",
       "       insult  racism  misogyny  xenophobia  hate  \n",
       "0         2.0     0.0       0.0         0.0     1  \n",
       "1         1.0     0.0       0.0         0.0     1  \n",
       "2         1.0     0.0       0.0         0.0     1  \n",
       "3         0.0     0.0       0.0         0.0     1  \n",
       "4         2.0     0.0       0.0         0.0     1  \n",
       "...       ...     ...       ...         ...   ...  \n",
       "20995     0.0     0.0       0.0         0.0     0  \n",
       "20996     0.0     0.0       0.0         0.0     1  \n",
       "20997     0.0     0.0       0.0         0.0     0  \n",
       "20998     0.0     0.0       0.0         0.0     0  \n",
       "20999     0.0     0.0       0.0         0.0     0  \n",
       "\n",
       "[21000 rows x 8 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"Portuguese\", \"Portuguese_1_train.csv\"))\n",
    "target_cols = ['homophobia', 'obscene', 'insult', 'racism', 'misogyny',\n",
    "       'xenophobia']\n",
    "df[\"hate\"]= df[target_cols].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)\n",
    "df\n",
    "# sum(df[\"hate\"])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, rest_df = train_test_split(df, test_size=0.2, stratify=df[\"hate\"], random_state=42)\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5, stratify=rest_df[\"hate\"], random_state=42)\n",
    "train_df[[\"text\", \"hate\"]].dropna().to_csv(os.path.join(DATA_DIR, \"Portuguese\", \"train.csv\"), index=False)\n",
    "val_df[[\"text\", \"hate\"]].dropna().to_csv(os.path.join(DATA_DIR, \"Portuguese\", \"val.csv\"), index=False)\n",
    "test_df[[\"text\", \"hate\"]].dropna().to_csv(os.path.join(DATA_DIR, \"Portuguese\", \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_portuguese_data():\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Portuguese\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Portuguese\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Portuguese\", \"val.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"val\": valid_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"Turkish\", \"Turkish_1_train.csv\"))\n",
    "train_df.rename(columns={\"subtask_a\": \"offensive\"}, inplace=True)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.11, stratify=train_df[\"offensive\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"Turkish\", \"Turkish_1_test.csv\"))\n",
    "test_df.rename(columns={\"subtask_a\": \"offensive\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna().to_csv(os.path.join(DATA_DIR, \"Turkish\", \"train.csv\"), index=False)\n",
    "val_df.dropna().to_csv(os.path.join(DATA_DIR, \"Turkish\", \"val.csv\"), index=False)\n",
    "test_df.dropna().to_csv(os.path.join(DATA_DIR, \"Turkish\", \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_turkish_data():\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Turkish\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Turkish\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Turkish\", \"val.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"val\": valid_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ukraninan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"Ukranian\", \"Ukranian_1_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"abusive\"] = df[\"abusive\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'video_id', 'author', 'text', 'abusive'], dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\", \"abusive\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, rest_df = train_test_split(df, test_size=0.2, stratify=df[\"abusive\"], random_state=42)\n",
    "val_df, test_df = train_test_split(rest_df, test_size=0.5, stratify=rest_df[\"abusive\"], random_state=42)\n",
    "\n",
    "train_df.dropna().to_csv(os.path.join(DATA_DIR, \"Ukranian\", \"train.csv\"), index=False)\n",
    "val_df.dropna().to_csv(os.path.join(DATA_DIR, \"Ukranian\", \"val.csv\"), index=False)\n",
    "test_df.dropna().to_csv(os.path.join(DATA_DIR, \"Ukranian\", \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ukranian_data():\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Ukranian\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Ukranian\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Ukranian\", \"val.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"val\": valid_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"Hindi\", \"Hindi_1_train.csv\")).rename(columns={\"Post\":\"text\"})\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"Hindi\", \"Hindi_1_test.csv\")).rename(columns={\"Post\":\"text\"})\n",
    "val_df = pd.read_csv(os.path.join(DATA_DIR, \"Hindi\", \"Hindi_1_val.csv\")).rename(columns={\"Post\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06654567453115548"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"hate\"] = [1 if \"hate\" in x else 0 for x in train_df[\"Labels Set\"]]\n",
    "test_df[\"hate\"] = [1 if \"hate\" in x else 0 for x in test_df[\"Labels Set\"]]\n",
    "val_df[\"hate\"] = [1 if \"hate\" in x else 0 for x in val_df[\"Labels Set\"]]\n",
    "sum(train_df[\"hate\"])/len(train_df)\n",
    "sum(val_df[\"hate\"])/len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"text\", \"hate\"]].to_csv(os.path.join(DATA_DIR, \"Hindi\", \"train.csv\"), index=False)\n",
    "test_df[[\"text\", \"hate\"]].to_csv(os.path.join(DATA_DIR, \"Hindi\", \"test.csv\"), index=False)\n",
    "val_df[[\"text\", \"hate\"]].to_csv(os.path.join(DATA_DIR, \"Hindi\", \"val.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hindi_data():\n",
    "    train_data = pd.read_csv(os.path.join(DATA_DIR, \"Hindi\", \"train.csv\"))\n",
    "    test_data = pd.read_csv(os.path.join(DATA_DIR, \"Hindi\", \"test.csv\"))\n",
    "    valid_data = pd.read_csv(os.path.join(DATA_DIR, \"Hindi\", \"val.csv\"))\n",
    "    return {\"train\": train_data, \"test\": test_data, \"val\": valid_data}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noise_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
